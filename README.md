# IBM Machine Learning Engineering Capstone

![IBM](https://img.shields.io/badge/IBM-052FAD?style=flat&logo=ibm&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)
![MLflow](https://img.shields.io/badge/MLflow-0194E2?style=flat&logo=mlflow&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&logo=docker&logoColor=white)
![Kubernetes](https://img.shields.io/badge/Kubernetes-326CE5?style=flat&logo=kubernetes&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

Projeto Capstone do **IBM Machine Learning Engineering Professional Certificate** - Plataforma enterprise de MLOps com pipelines automatizados, model registry, monitoring e serving de modelos em produÃ§Ã£o.

## ğŸ¯ VisÃ£o Geral

Sistema completo de MLOps que demonstra competÃªncias avanÃ§adas em engenharia de machine learning, deployment de modelos e operaÃ§Ãµes de ML em ambiente de produÃ§Ã£o enterprise.

### âœ¨ CaracterÃ­sticas Principais

- **ğŸ”„ MLOps Pipeline**: Treinamento, validaÃ§Ã£o e deploy automatizados
- **ğŸ“¦ Model Registry**: Versionamento e gestÃ£o centralizada de modelos
- **ğŸš€ Model Serving**: APIs de produÃ§Ã£o com alta disponibilidade
- **ğŸ“Š Monitoring**: DetecÃ§Ã£o de drift e monitoramento de performance
- **ğŸ³ ContainerizaÃ§Ã£o**: Docker e Kubernetes para escalabilidade
- **âš¡ CI/CD**: Pipeline de integraÃ§Ã£o e deployment contÃ­nuo

## ğŸ› ï¸ Stack TecnolÃ³gico

### MLOps & Engineering
- **MLflow**: Experiment tracking e model registry
- **Docker**: ContainerizaÃ§Ã£o de modelos
- **Kubernetes**: OrquestraÃ§Ã£o e auto-scaling
- **FastAPI**: APIs de serving de alta performance
- **Prometheus/Grafana**: Monitoring e observabilidade

### Machine Learning
- **TensorFlow**: Deep learning e neural networks
- **Scikit-learn**: Machine learning clÃ¡ssico
- **XGBoost**: Gradient boosting otimizado
- **Pandas/NumPy**: Processamento de dados

### Infrastructure
- **PostgreSQL**: Metadata e model registry
- **Redis**: Cache e feature store
- **NGINX**: Load balancing e reverse proxy

## ğŸ“ Estrutura do Projeto

```
ibm-machine-learning-capstone/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                   # Pipeline de dados
â”‚   â”œâ”€â”€ models/                 # Modelos de ML
â”‚   â”œâ”€â”€ serving/                # APIs de serving
â”‚   â”œâ”€â”€ monitoring/             # Monitoring e alertas
â”‚   â””â”€â”€ utils/                  # UtilitÃ¡rios
â”œâ”€â”€ tests/                      # Testes automatizados
â”œâ”€â”€ k8s/                        # Manifests Kubernetes
â”œâ”€â”€ docker/                     # Dockerfiles
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â””â”€â”€ README.md                   # DocumentaÃ§Ã£o
```

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Python 3.11+
- Docker & Docker Compose
- Kubernetes (opcional)

### InstalaÃ§Ã£o Local

1. **Clone o repositÃ³rio:**
```bash
git clone https://github.com/galafis/ibm-machine-learning-capstone.git
cd ibm-machine-learning-capstone
```

2. **Configure o ambiente:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt
```

3. **Execute com Docker:**
```bash
docker-compose up -d
```

4. **Acesse a plataforma:**
```
http://localhost:8080
```

## ğŸ”¬ MLOps Pipeline

### 1. Experiment Tracking
```python
import mlflow
import mlflow.sklearn

with mlflow.start_run():
    # Treinar modelo
    model = train_model(X_train, y_train)
    
    # Log parÃ¢metros e mÃ©tricas
    mlflow.log_params({"n_estimators": 100, "max_depth": 10})
    mlflow.log_metrics({"accuracy": 0.95, "f1_score": 0.93})
    
    # Registrar modelo
    mlflow.sklearn.log_model(model, "fraud_detection_model")
```

### 2. Model Serving
```python
from fastapi import FastAPI
import joblib

app = FastAPI()
model = joblib.load("models/production_model.pkl")

@app.post("/predict")
async def predict(data: PredictionRequest):
    prediction = model.predict(data.features)
    confidence = model.predict_proba(data.features).max()
    
    return {
        "prediction": prediction.tolist(),
        "confidence": float(confidence)
    }
```

### 3. Model Monitoring
```python
from evidently import ColumnMapping
from evidently.report import Report
from evidently.metric_preset import DataDriftPreset

# Detectar data drift
report = Report(metrics=[DataDriftPreset()])
report.run(reference_data=reference_df, current_data=current_df)

if report.get_metric("DatasetDriftMetric").drift_detected:
    trigger_retraining_pipeline()
```

## ğŸ¤– Modelos Implementados

### Fraud Detection
```python
import xgboost as xgb

fraud_model = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    eval_metric='auc'
)
```

### Recommendation System
```python
from sklearn.decomposition import NMF

recommendation_model = NMF(
    n_components=50,
    init='random',
    random_state=42
)
```

### Time Series Forecasting
```python
import tensorflow as tf

lstm_model = tf.keras.Sequential([
    tf.keras.layers.LSTM(50, return_sequences=True),
    tf.keras.layers.LSTM(50),
    tf.keras.layers.Dense(1)
])
```

## ğŸ“Š MÃ©tricas de Performance

### Targets de ProduÃ§Ã£o
- **LatÃªncia**: < 100ms por prediÃ§Ã£o
- **Throughput**: > 1000 requests/segundo
- **Accuracy**: > 95% nos modelos de classificaÃ§Ã£o
- **Uptime**: 99.9% disponibilidade

### Monitoramento em Tempo Real
```python
from prometheus_client import Counter, Histogram

prediction_counter = Counter('ml_predictions_total', 'Total predictions')
prediction_latency = Histogram('ml_prediction_duration_seconds', 'Prediction latency')

@prediction_latency.time()
def make_prediction(data):
    prediction_counter.inc()
    return model.predict(data)
```

## ğŸ³ Deploy com Kubernetes

### Deploy da AplicaÃ§Ã£o
```bash
# Deploy completo
kubectl apply -f k8s/

# Verificar status
kubectl get pods -n ml-platform

# Acessar via port-forward
kubectl port-forward svc/ml-serving 8080:80
```

### Auto-scaling
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-serving-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-serving
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
```

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Executar Testes
```bash
# Testes unitÃ¡rios
pytest tests/unit/

# Testes de integraÃ§Ã£o
pytest tests/integration/

# Testes de performance
pytest tests/performance/
```

### ValidaÃ§Ã£o de Modelos
```bash
# ValidaÃ§Ã£o cruzada
python src/models/validate_model.py --model fraud_detection

# Backtesting
python scripts/backtest.py --start-date 2024-01-01
```

## ğŸ“ˆ Casos de Uso Enterprise

### 1. DetecÃ§Ã£o de Fraudes
- Processamento em tempo real de transaÃ§Ãµes
- Alertas automÃ¡ticos para atividades suspeitas
- ReduÃ§Ã£o de 80% em falsos positivos

### 2. Sistema de RecomendaÃ§Ã£o
- RecomendaÃ§Ãµes personalizadas em tempo real
- A/B testing para otimizaÃ§Ã£o contÃ­nua
- Aumento de 25% no engagement

### 3. ManutenÃ§Ã£o Preditiva
- PrevisÃ£o de falhas em equipamentos
- OtimizaÃ§Ã£o de cronogramas de manutenÃ§Ã£o
- ReduÃ§Ã£o de 40% em downtime nÃ£o planejado

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### MLflow Configuration
```python
# mlflow_config.py
MLFLOW_CONFIG = {
    'tracking_uri': 'postgresql://user:pass@localhost/mlflow',
    'artifact_root': 's3://ml-artifacts-bucket',
    'experiment_name': 'production_models'
}
```

### Model Registry
```python
# Promover modelo para produÃ§Ã£o
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="fraud_detection_model",
    version=3,
    stage="Production"
)
```

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ‘¨â€ğŸ’» Autor

**Gabriel Demetrios Lafis**

- **CertificaÃ§Ã£o**: IBM Machine Learning Engineering Professional Certificate
- GitHub: [@galafis](https://github.com/galafis)
- Email: gabrieldemetrios@gmail.com

---

â­ Se este projeto foi Ãºtil, considere deixar uma estrela!

