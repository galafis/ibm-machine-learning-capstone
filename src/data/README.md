# Data Pipeline

![Data Engineering](https://img.shields.io/badge/Data-Engineering-blue) ![MLOps](https://img.shields.io/badge/MLOps-Pipeline-green) ![Quality](https://img.shields.io/badge/Data%20Quality-Validated-success)

Este mÃ³dulo contÃ©m o pipeline de dados da plataforma MLOps, responsÃ¡vel pela ingestÃ£o, processamento, validaÃ§Ã£o e preparaÃ§Ã£o de dados para treinamento e inferÃªncia de modelos de machine learning.

## ğŸ¯ Objetivos

- **IngestÃ£o EscalÃ¡vel**: Processamento de dados em lote e tempo real
- **Qualidade de Dados**: ValidaÃ§Ã£o e limpeza automatizada
- **Feature Engineering**: TransformaÃ§Ãµes e criaÃ§Ã£o de features
- **Versionamento**: Controle de versÃ£o de datasets e schemas
- **Monitoramento**: Observabilidade completa do pipeline

## ğŸ“Š Estrutura dos Dados

```
src/data/
â”œâ”€â”€ ingestion/          # MÃ³dulos de ingestÃ£o de dados
â”‚   â”œâ”€â”€ batch/          # Processamento em lote
â”‚   â”œâ”€â”€ streaming/      # Processamento em tempo real
â”‚   â””â”€â”€ connectors/     # Conectores para fontes externas
â”œâ”€â”€ processing/         # Pipeline de processamento
â”‚   â”œâ”€â”€ validation/     # ValidaÃ§Ã£o de qualidade
â”‚   â”œâ”€â”€ cleaning/       # Limpeza e tratamento
â”‚   â””â”€â”€ transformation/ # TransformaÃ§Ãµes e features
â”œâ”€â”€ storage/            # Camada de armazenamento
â”‚   â”œâ”€â”€ raw/           # Dados brutos
â”‚   â”œâ”€â”€ processed/     # Dados processados
â”‚   â””â”€â”€ features/      # Feature store
â””â”€â”€ monitoring/         # Monitoramento de dados
    â”œâ”€â”€ quality/       # MÃ©tricas de qualidade
    â”œâ”€â”€ drift/         # DetecÃ§Ã£o de drift
    â””â”€â”€ lineage/       # Linhagem de dados
```

## ğŸ”„ Pipeline de Dados

### 1. IngestÃ£o

```python
from src.data.ingestion import DataIngestion

# IngestÃ£o em lote
batch_ingestion = DataIngestion(
    source="s3://raw-data-bucket/",
    format="parquet",
    schedule="@daily"
)

# IngestÃ£o em tempo real
streaming_ingestion = DataIngestion(
    source="kafka://ml-events",
    format="json",
    window="5min"
)
```

### 2. ValidaÃ§Ã£o de Qualidade

```python
from src.data.processing.validation import DataValidator

validator = DataValidator()

# Definir regras de validaÃ§Ã£o
validation_rules = {
    "completeness": {"threshold": 0.95},
    "uniqueness": {"columns": ["transaction_id"]},
    "range": {"amount": {"min": 0, "max": 100000}}
}

# Executar validaÃ§Ã£o
results = validator.validate(data, validation_rules)
```

### 3. Feature Engineering

```python
from src.data.processing.transformation import FeatureEngineer

feature_engineer = FeatureEngineer()

# TransformaÃ§Ãµes automÃ¡ticas
features = feature_engineer.transform({
    "categorical_encoding": "target",
    "numerical_scaling": "robust",
    "feature_selection": "mutual_info",
    "temporal_features": ["hour", "day_of_week"]
})
```

## ğŸ“ˆ Fontes de Dados Suportadas

### Dados Estruturados
- **TransaÃ§Ãµes Financeiras**: DetecÃ§Ã£o de fraudes
- **Comportamento de UsuÃ¡rios**: Sistema de recomendaÃ§Ã£o
- **Sensores IoT**: ManutenÃ§Ã£o preditiva
- **Logs de Sistema**: Monitoramento operacional

### Formatos Suportados
- **Batch**: Parquet, CSV, JSON, Avro
- **Streaming**: Kafka, Kinesis, PubSub
- **APIs**: REST, GraphQL
- **Databases**: PostgreSQL, MongoDB, Cassandra

## ğŸ›ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# Storage
DATA_LAKE_BUCKET=s3://ml-data-lake
FEATURE_STORE_URI=postgresql://localhost:5432/features

# Streaming
KAFKA_BROKERS=localhost:9092
STREAM_PROCESSING_MODE=exactly_once

# Qualidade
DATA_QUALITY_THRESHOLD=0.95
DRIFT_DETECTION_WINDOW=7d
```

### ConfiguraÃ§Ã£o do Pipeline

```yaml
# data_pipeline.yml
pipeline:
  ingestion:
    batch_size: 10000
    parallel_workers: 4
    retry_policy: exponential_backoff
  
  validation:
    enable_profiling: true
    fail_on_error: false
    quality_threshold: 0.95
  
  processing:
    cache_intermediate: true
    optimize_memory: true
    enable_monitoring: true
```

## ğŸ” Monitoramento e Observabilidade

### MÃ©tricas Principais
- **Volume**: Registros processados por hora
- **Qualidade**: Taxa de aprovaÃ§Ã£o nas validaÃ§Ãµes
- **LatÃªncia**: Tempo de processamento end-to-end
- **Drift**: Desvio estatÃ­stico dos dados

### Alertas AutomÃ¡ticos
- **Falha na IngestÃ£o**: > 5% de falha em 1h
- **Qualidade Baixa**: < 90% de aprovaÃ§Ã£o
- **Drift Detectado**: Desvio > 2 desvios padrÃ£o
- **LatÃªncia Alta**: > 10min para processamento

## ğŸ§ª Testes e ValidaÃ§Ã£o

### Executar Testes

```bash
# Testes unitÃ¡rios do pipeline
pytest tests/data/unit/

# Testes de integraÃ§Ã£o
pytest tests/data/integration/

# Testes de qualidade de dados
pytest tests/data/quality/
```

### ValidaÃ§Ã£o de Schema

```python
from src.data.validation import SchemaValidator

# Definir schema esperado
schema = {
    "user_id": "int64",
    "transaction_amount": "float64",
    "timestamp": "datetime64[ns]",
    "merchant_category": "category"
}

# Validar dados contra schema
validator = SchemaValidator(schema)
results = validator.validate(df)
```

## ğŸ“ Logs e Auditoria

### Estrutura de Logs

```json
{
  "timestamp": "2024-09-15T14:30:00Z",
  "level": "INFO",
  "component": "data.ingestion.batch",
  "message": "Processed 10000 records successfully",
  "metadata": {
    "source": "transactions_2024_09_15.parquet",
    "records_processed": 10000,
    "processing_time_ms": 1500,
    "data_quality_score": 0.97
  }
}
```

### Linhagem de Dados

- **Rastreabilidade**: Origem atÃ© modelo final
- **TransformaÃ§Ãµes**: Log de todas as operaÃ§Ãµes
- **Versionamento**: Controle de versÃ£o de datasets
- **Impacto**: AnÃ¡lise de downstream dependencies

## ğŸš€ Deploy e ProduÃ§Ã£o

### ContainerizaÃ§Ã£o

```dockerfile
# Dockerfile.data-pipeline
FROM python:3.11-slim

WORKDIR /app
COPY requirements-data.txt .
RUN pip install -r requirements-data.txt

COPY src/data/ ./src/data/
CMD ["python", "-m", "src.data.main"]
```

### Kubernetes Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-pipeline
spec:
  replicas: 3
  selector:
    matchLabels:
      app: data-pipeline
  template:
    spec:
      containers:
      - name: data-pipeline
        image: ml-platform/data-pipeline:latest
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
```

## ğŸ“š DocumentaÃ§Ã£o Adicional

- [Guia de IngestÃ£o de Dados](./docs/ingestion-guide.md)
- [PadrÃµes de Qualidade](./docs/quality-standards.md)
- [Feature Engineering](./docs/feature-engineering.md)
- [Troubleshooting](./docs/troubleshooting.md)

## ğŸ¤ ContribuiÃ§Ã£o

Para contribuir com melhorias no pipeline de dados:

1. Fork o repositÃ³rio
2. Crie uma branch para sua feature: `git checkout -b feature/data-enhancement`
3. Implemente suas mudanÃ§as seguindo os padrÃµes de qualidade
4. Execute todos os testes: `make test-data`
5. Submeta um Pull Request

---

**Mantido pela equipe de Data Engineering**  
ğŸ“§ [data-engineering@company.com](mailto:data-engineering@company.com)  
ğŸ“‹ [Roadmap do Pipeline](https://github.com/galafis/ibm-machine-learning-capstone/projects/data-pipeline)
